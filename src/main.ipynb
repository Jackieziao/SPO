{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function_library import *\n",
    "from reformulation_library import *\n",
    "from main_library import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lambda=10\n",
    "lambda_max = None \n",
    "lambda_min_ratio = 0.0001\n",
    "regularization = 'ridge'\n",
    "\n",
    "different_validation_losses = False\n",
    "n_holdout = 1000\n",
    "n_test = 1000\n",
    "grid_dim = 5\n",
    "p_features = 5\n",
    "n_train = 1000\n",
    "polykernel_degree = 1\n",
    "polykernel_noise_half_width = 0\n",
    "d_feasibleregion = 2 * p_features * (p_features - 1)\n",
    "sources, destinations = convert_grid_to_list(grid_dim, grid_dim)\n",
    "sp_graph = shortest_path_graph(sources = sources, destinations = destinations,\n",
    "        start_node = 1, end_node = grid_dim^2, acyclic = True)\n",
    "B_true = np.array([[bernoulli(0.5) for k in range(p_features)] for e in range(d_feasibleregion)])\n",
    "X_train, c_train = generate_poly_kernel_data_simple(B_true, n_train, polykernel_degree, polykernel_noise_half_width)\n",
    "X_validation, c_validation = generate_poly_kernel_data_simple(B_true, n_holdout, polykernel_degree, polykernel_noise_half_width)\n",
    "X_test, c_test = generate_poly_kernel_data_simple(B_true, n_test, polykernel_degree, polykernel_noise_half_width)\n",
    "# Add intercept in the first row of X\n",
    "# an intercept term is a constant term that is added to a linear model to \n",
    "# account for the mean or baseline level of the target variable.\n",
    "# X_train = np.vstack((np.ones(n_train), X_train))\n",
    "# X_validation = np.vstack((np.ones(n_holdout), X_validation))\n",
    "# X_test = np.vstack((np.ones(n_test), X_test))\n",
    "\n",
    "# Solve the shortest path problem\n",
    "G = define_graph(grid_dim, showflag=False)\n",
    "solver = Shortest_path_solver(G)\n",
    "z_train, w_train = batch_solve(solver, c_train)\n",
    "z_validation, w_validation = batch_solve(solver, c_validation)\n",
    "z_test, w_test = batch_solve(solver, c_test)\n",
    "\n",
    "c_ham_train = np.ones((d_feasibleregion, n_train)) - w_train\n",
    "c_ham_validation = np.ones((d_feasibleregion, n_holdout)) - w_validation\n",
    "c_ham_test = np.ones((d_feasibleregion, n_test)) - w_test\n",
    "\n",
    "# Put train + validation together\n",
    "X_both = np.hstack((X_train, X_validation))\n",
    "c_both = np.hstack((c_train, c_validation))\n",
    "c_ham_both = np.hstack((c_ham_train, c_ham_validation))\n",
    "train_index = np.arange(n_train)\n",
    "validation_index = np.arange(n_train, n_train + n_holdout)\n",
    "\n",
    "# Set validation losses\n",
    "if different_validation_losses:\n",
    "    spo_plus_val_loss = 'spo_loss'\n",
    "    ls_val_loss = 'least_squares_loss'\n",
    "    absolute_val_loss = 'absolute_loss'\n",
    "else:\n",
    "    spo_plus_val_loss = 'spo_loss'\n",
    "    ls_val_loss = 'spo_loss'\n",
    "    absolute_val_loss = 'spo_loss'\n",
    "### Algorithms ###\n",
    "\n",
    "# SPO+\n",
    "best_B_SPOplus, best_lambda_SPOplus = validation_set_alg(X_train, c_train, X_validation, c_validation, solver, sp_graph = sp_graph,\n",
    "    val_alg_parms = ValParms(algorithm_type = 'sp_spo_plus_reform', validation_loss = spo_plus_val_loss),\n",
    "    path_alg_parms = PathParms(num_lambda = num_lambda, lambda_max = lambda_max, \n",
    "                                            regularization = regularization,\n",
    "                                            lambda_min_ratio = lambda_min_ratio, algorithm_type = 'SPO_plus'))\n",
    "best_B_leastSquares, best_lambda_leastSquares = validation_set_alg(X_train, c_train, X_validation, c_validation, solver, sp_graph = sp_graph,\n",
    "    val_alg_parms = ValParms(algorithm_type = 'ls_jump', validation_loss = ls_val_loss),\n",
    "    path_alg_parms = PathParms(num_lambda = num_lambda, lambda_max = lambda_max, \n",
    "                                            regularization = regularization,\n",
    "                                            lambda_min_ratio = lambda_min_ratio, algorithm_type = 'leastSquares'))\n",
    "best_B_leastSquares, best_lambda_leastSquares = validation_set_alg(X_train, c_train, X_validation, c_validation, solver, sp_graph = sp_graph,\n",
    "    val_alg_parms = ValParms(algorithm_type = 'ls_jump', validation_loss = absolute_val_loss),\n",
    "    path_alg_parms = PathParms(num_lambda = num_lambda, lambda_max = lambda_max, \n",
    "                                            regularization = regularization,\n",
    "                                            po_loss_function = 'absolute',\n",
    "                                            lambda_min_ratio = lambda_min_ratio, algorithm_type = 'Absolute'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shortest_path_multiple_replications(rng_seed, num_trials, grid_dim, n_train_vec, n_test, p_features, polykernel_degree_vec, polykernel_noise_half_width_vec, num_lambda=10, lambda_max=None, lambda_min_ratio=0.0001, holdout_percent=0.25, regularization='ridge', different_validation_losses=False, include_rf=True):\n",
    "    np.random.seed(rng_seed)\n",
    "    big_results_df = make_blank_complete_df()\n",
    "\n",
    "    for n_train in n_train_vec:\n",
    "        for polykernel_degree in polykernel_degree_vec:\n",
    "            for polykernel_noise_half_width in polykernel_noise_half_width_vec:\n",
    "                print(f\"Moving on to n_train = {n_train}, polykernel_degree = {polykernel_degree}, polykernel_noise_half_width = {polykernel_noise_half_width}\")\n",
    "                for trial in range(num_trials):\n",
    "                    print(f\"Current trial is {trial}\")\n",
    "                    n_holdout = round(holdout_percent*n_train)\n",
    "\n",
    "                    current_results = shortest_path_replication(grid_dim,\n",
    "                        n_train, n_holdout, n_test,\n",
    "                        p_features, polykernel_degree, polykernel_noise_half_width,\n",
    "                        num_lambda=num_lambda, lambda_max=lambda_max, lambda_min_ratio=lambda_min_ratio,\n",
    "                        regularization=regularization,\n",
    "                        different_validation_losses=different_validation_losses,\n",
    "                        include_rf=include_rf)\n",
    "\n",
    "                    current_df_row = build_complete_row(grid_dim,\n",
    "                        n_train, n_holdout, n_test,\n",
    "                        p_features, polykernel_degree, polykernel_noise_half_width, current_results)\n",
    "\n",
    "                    big_results_df = pd.concat([big_results_df, current_df_row])\n",
    "\n",
    "    return big_results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envtwo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
