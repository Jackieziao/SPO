{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('shortest_path_100_1000_o.csv')\n",
    "\n",
    "# select the rows to keep using iloc\n",
    "rows_to_keep = [1, 2, 3] + [i for i in range(11, len(df)) if (i % 10) in [1, 2, 3]]\n",
    "df = df.iloc[rows_to_keep]\n",
    "\n",
    "# write the result to a new CSV file\n",
    "df.to_csv('shortest_path_100_1000.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the two CSV files into Pandas DataFrames\n",
    "df1 = pd.read_csv('shortest_path_1.csv')\n",
    "df2 = pd.read_csv('shortest_path_50000.csv')\n",
    "\n",
    "# merge the two DataFrames based on a common column\n",
    "merged_df = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# write the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('shortest_path.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Process\n",
    "results = pd.read_csv(\"shortest_path.csv\")\n",
    "results = results.loc[(results[\"grid_dim\"] == 5) & (results[\"p_features\"] == 5)]\n",
    "results_summary = results.describe()\n",
    "results = results.assign(\n",
    "    SPOplus_norm_spo=results[\"SPOplus_spoloss_test\"] / results[\"zstar_avg_test\"],\n",
    "    LS_norm_spo=results[\"LS_spoloss_test\"] / results[\"zstar_avg_test\"],\n",
    "    RF_norm_spo=results[\"RF_spoloss_test\"] / results[\"zstar_avg_test\"],\n",
    "    Absolute_norm_spo=results[\"Absolute_spoloss_test\"] / results[\"zstar_avg_test\"]\n",
    ")\n",
    "results = results.rename(columns={\n",
    "    \"SPOplus_norm_spo\": \"SPO+\",\n",
    "    \"LS_norm_spo\": \"Least Squares\",\n",
    "    \"RF_norm_spo\": \"Random Forests\",\n",
    "    \"Absolute_norm_spo\": \"Absolute Loss\"\n",
    "})\n",
    "results_relevant = results.filter([\"grid_dim\", \"p_features\", \"n_train\", \"polykernel_degree\", \n",
    "                                   \"polykernel_noise_half_width\", \"SPO+\", \"Least Squares\", \"Random Forests\", \"Absolute Loss\"])\n",
    "results_relevant_fixed = results_relevant.melt(id_vars=[\"grid_dim\", \"p_features\", \"n_train\", \n",
    "                                                        \"polykernel_degree\", \"polykernel_noise_half_width\"], var_name=\"method\", value_name=\"spo_normalized\")\n",
    "results_relevant_fixed[\"method\"] = pd.Categorical(results_relevant_fixed[\"method\"])\n",
    "results_relevant_fixed[\"n_train\"] = pd.Categorical(results_relevant_fixed[\"n_train\"])\n",
    "results_relevant_fixed[\"polykernel_noise_half_width\"] = pd.Categorical(results_relevant_fixed[\"polykernel_noise_half_width\"])\n",
    "results_relevant_fixed[\"grid_dim\"] = pd.Categorical(results_relevant_fixed[\"grid_dim\"])\n",
    "training_set_size_names = [\n",
    "    \"Training Set Size = 100\",\n",
    "    \"Training Set Size = 1000\",\n",
    "    \"Training Set Size = 5000\"\n",
    "]\n",
    "half_width_names = [\"Noise Half-width = 0\", \"Noise Half-width = 0.5\"]\n",
    "training_set_size_names = {\n",
    "    100: \"Training Set Size = 100\",\n",
    "    1000: \"Training Set Size = 1000\",\n",
    "    5000: \"Training Set Size = 5000\"\n",
    "}\n",
    "half_width_names = {\n",
    "    0: \"Noise Half-width = 0\",\n",
    "    0.5: \"Noise Half-width = 0.5\"\n",
    "}\n",
    "training_set_size_labeller = lambda x: training_set_size_names[x]\n",
    "half_width_labeller = lambda x: half_width_names[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add facet wrap\n",
    "g = sns.FacetGrid(data=results_relevant_fixed, row='n_train', col='polykernel_noise_half_width', height=6, aspect=1.2)\n",
    "g.map_dataframe(sns.boxplot, x='polykernel_degree', y='spo_normalized', hue='method', palette=[\"pink\", \"palegreen\", \"skyblue\", \"lavender\"])\n",
    "g.set_yticklabels(['{:,.0%}'.format(x) for x in plt.gca().get_yticks()])\n",
    "g.set_xlabels(\"Polynomial Degree\")\n",
    "g.set_ylabels(\"Normalized SPO Loss\")\n",
    "g.set_titles(col_template=\"Noise Half-width = {row_name}\", row_template=\"Training Set Size = {col_name}\")\n",
    "g.set_axis_labels(x_var=\"Polynomial Degree\", y_var=\"Normalized SPO Loss\")\n",
    "g.add_legend(title=\"Method\", bbox_to_anchor=(0.32, 1.04), loc='upper center', ncol=4, mode='fill')\n",
    "g.fig.suptitle('Normalized SPO Loss vs. Polynomial Degree', fontsize=24, y=1.05, x=0.32)\n",
    "# Save plot\n",
    "g.savefig(\"shotest_path_plot.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envtwo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
